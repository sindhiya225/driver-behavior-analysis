{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "-096FPSGRZzA",
        "outputId": "5099bed0-1fbe-4777-9515-31eea78e585f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-2308304716.py, line 688)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2308304716.py\"\u001b[0;36m, line \u001b[0;32m688\u001b[0m\n\u001b[0;31m    print(key_stats\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# # Driver Behavior Analysis - Feature Engineering\n",
        "#\n",
        "# ## Overview\n",
        "# This notebook focuses on creating meaningful features from raw telematics data for driver behavior analysis.\n",
        "#\n",
        "# ### Objectives:\n",
        "# 1. Extract time-based behavior patterns\n",
        "# 2. Calculate acceleration/deceleration metrics\n",
        "# 3. Derive speed and RPM features\n",
        "# 4. Create composite safety and efficiency scores\n",
        "# 5. Prepare data for clustering analysis\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 1. Setup and Configuration\n",
        "\n",
        "# %%\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.signal import savgol_filter\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.max_rows', 50)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"viridis\")\n",
        "\n",
        "# Import custom modules\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from src.feature_extractor import FeatureExtractor\n",
        "from src.data_processor import DataProcessor\n",
        "\n",
        "# %%\n",
        "# Load configuration\n",
        "import yaml\n",
        "with open('../config/config.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Load processed data\n",
        "print(\"Loading processed data...\")\n",
        "processor = DataProcessor('../config/config.yaml')\n",
        "df = processor.load_data()\n",
        "df = processor.calculate_basic_metrics(df)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Number of drivers: {len(df)}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 2. Time-Based Feature Extraction\n",
        "\n",
        "# %%\n",
        "# Initialize feature extractor\n",
        "extractor = FeatureExtractor()\n",
        "\n",
        "# Extract time-based features\n",
        "print(\"Extracting time-based features...\")\n",
        "df_time = extractor.extract_time_based_features(df.copy())\n",
        "\n",
        "# Display time features\n",
        "time_features = [col for col in df_time.columns if 'time' in col.lower()]\n",
        "print(f\"\\nTime-based features created: {len(time_features)}\")\n",
        "print(\"\\nSample time features:\")\n",
        "print(df_time[time_features].describe().round(3).T.head(10))\n",
        "\n",
        "# %%\n",
        "# Visualize time-based features\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "# Time response distribution\n",
        "axes[0, 0].hist(df_time['time_response_mean'].dropna(), bins=30, alpha=0.7)\n",
        "axes[0, 0].axvline(df_time['time_response_mean'].mean(), color='red', linestyle='--', label=f'Mean: {df_time[\"time_response_mean\"].mean():.2f}')\n",
        "axes[0, 0].set_xlabel('Response Time (seconds)')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].set_title('Driver Response Time Distribution')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Time consistency score\n",
        "axes[0, 1].hist(df_time['time_consistency'].dropna(), bins=30, alpha=0.7)\n",
        "axes[0, 1].axvline(df_time['time_consistency'].mean(), color='red', linestyle='--', label=f'Mean: {df_time[\"time_consistency\"].mean():.2f}')\n",
        "axes[0, 1].set_xlabel('Time Consistency Score')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title('Time Consistency Distribution')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Response time vs consistency scatter\n",
        "scatter = axes[0, 2].scatter(df_time['time_response_mean'], df_time['time_consistency'],\n",
        "                             alpha=0.6, c=df_time['time_response_std'], cmap='viridis')\n",
        "axes[0, 2].set_xlabel('Mean Response Time')\n",
        "axes[0, 2].set_ylabel('Time Consistency')\n",
        "axes[0, 2].set_title('Response Time vs Consistency')\n",
        "plt.colorbar(scatter, ax=axes[0, 2], label='Response Time STD')\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "# Percentile analysis\n",
        "time_percentiles = ['time_p95', 'time_p99']\n",
        "percentile_data = df_time[time_percentiles]\n",
        "box_data = [percentile_data[col].dropna() for col in time_percentiles]\n",
        "axes[1, 0].boxplot(box_data, labels=['95th %ile', '99th %ile'])\n",
        "axes[1, 0].set_ylabel('Time (seconds)')\n",
        "axes[1, 0].set_title('Response Time Percentiles')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Correlation heatmap\n",
        "time_corr_features = ['time_response_mean', 'time_response_std', 'time_reaction_mean',\n",
        "                      'time_reaction_std', 'time_consistency', 'time_p95', 'time_p99']\n",
        "time_corr = df_time[time_corr_features].corr()\n",
        "sns.heatmap(time_corr, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "            square=True, ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Time Feature Correlations')\n",
        "\n",
        "# Time variability distribution\n",
        "axes[1, 2].hist(df_time['time_response_std'].dropna(), bins=30, alpha=0.7)\n",
        "axes[1, 2].axvline(df_time['time_response_std'].mean(), color='red', linestyle='--',\n",
        "                   label=f'Mean: {df_time[\"time_response_std\"].mean():.2f}')\n",
        "axes[1, 2].set_xlabel('Response Time STD')\n",
        "axes[1, 2].set_ylabel('Frequency')\n",
        "axes[1, 2].set_title('Response Time Variability')\n",
        "axes[1, 2].legend()\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../results/features/time_features_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 3. Acceleration Feature Extraction\n",
        "\n",
        "# %%\n",
        "# Extract acceleration features\n",
        "print(\"Extracting acceleration features...\")\n",
        "df_accel = extractor.extract_acceleration_features(df_time)\n",
        "\n",
        "# Display acceleration features\n",
        "accel_features = [col for col in df_accel.columns if any(x in col for x in ['accel', 'decel', 'jerk', 'harsh', 'smooth'])]\n",
        "print(f\"\\nAcceleration features created: {len(accel_features)}\")\n",
        "print(\"\\nSample acceleration features:\")\n",
        "print(df_accel[accel_features].describe().round(3).T.head(15))\n",
        "\n",
        "# %%\n",
        "# Visualize acceleration features\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "# Harsh acceleration count distribution\n",
        "axes[0, 0].hist(df_accel['harsh_accel_count'].dropna(), bins=30, alpha=0.7)\n",
        "axes[0, 0].axvline(df_accel['harsh_accel_count'].mean(), color='red', linestyle='--',\n",
        "                   label=f'Mean: {df_accel[\"harsh_accel_count\"].mean():.2f}')\n",
        "axes[0, 0].set_xlabel('Harsh Acceleration Count')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].set_title('Harsh Acceleration Distribution')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Harsh braking count distribution\n",
        "axes[0, 1].hist(df_accel['harsh_brake_count'].dropna(), bins=30, alpha=0.7)\n",
        "axes[0, 1].axvline(df_accel['harsh_brake_count'].mean(), color='red', linestyle='--',\n",
        "                   label=f'Mean: {df_accel[\"harsh_brake_count\"].mean():.2f}')\n",
        "axes[0, 1].set_xlabel('Harsh Braking Count')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title('Harsh Braking Distribution')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Aggressiveness vs smooth driving\n",
        "scatter = axes[0, 2].scatter(df_accel['accel_aggressiveness'], df_accel['smooth_driving_score'],\n",
        "                             alpha=0.6, c=df_accel['jerk_score'], cmap='plasma')\n",
        "axes[0, 2].set_xlabel('Acceleration Aggressiveness')\n",
        "axes[0, 2].set_ylabel('Smooth Driving Score')\n",
        "axes[0, 2].set_title('Aggressiveness vs Smooth Driving')\n",
        "plt.colorbar(scatter, ax=axes[0, 2], label='Jerk Score')\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "# Extreme events comparison\n",
        "extreme_events = ['very_harsh_accel_count', 'harsh_accel_count',\n",
        "                  'very_harsh_brake_count', 'harsh_brake_count']\n",
        "extreme_data = df_accel[extreme_events]\n",
        "box_data = [extreme_data[col].dropna() for col in extreme_events]\n",
        "axes[1, 0].boxplot(box_data, labels=['V.H. Accel', 'H. Accel', 'V.H. Brake', 'H. Brake'])\n",
        "axes[1, 0].set_ylabel('Event Count')\n",
        "axes[1, 0].set_title('Extreme Event Distribution')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Correlation heatmap\n",
        "accel_corr_features = ['accel_aggressiveness', 'decel_aggressiveness', 'jerk_score',\n",
        "                       'smooth_driving_score', 'harsh_accel_count', 'harsh_brake_count']\n",
        "accel_corr = df_accel[accel_corr_features].corr()\n",
        "sns.heatmap(accel_corr, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "            square=True, ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Acceleration Feature Correlations')\n",
        "\n",
        "# Smooth driving score distribution\n",
        "axes[1, 2].hist(df_accel['smooth_driving_score'].dropna(), bins=30, alpha=0.7)\n",
        "axes[1, 2].axvline(df_accel['smooth_driving_score'].mean(), color='red', linestyle='--',\n",
        "                   label=f'Mean: {df_accel[\"smooth_driving_score\"].mean():.2f}')\n",
        "axes[1, 2].set_xlabel('Smooth Driving Score')\n",
        "axes[1, 2].set_ylabel('Frequency')\n",
        "axes[1, 2].set_title('Smooth Driving Distribution')\n",
        "axes[1, 2].legend()\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../results/features/acceleration_features_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 4. Speed Feature Extraction\n",
        "\n",
        "# %%\n",
        "# Extract speed features\n",
        "print(\"Extracting speed features...\")\n",
        "df_speed = extractor.extract_speed_features(df_accel)\n",
        "\n",
        "# Display speed features\n",
        "speed_features = [col for col in df_speed.columns if 'speed' in col.lower() and col not in df.columns]\n",
        "print(f\"\\nSpeed features created: {len(speed_features)}\")\n",
        "print(\"\\nSample speed features:\")\n",
        "print(df_speed[speed_features].describe().round(3).T.head(15))\n",
        "\n",
        "# %%\n",
        "# Visualize speed features\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "# Average speed distribution\n",
        "axes[0, 0].hist(df_speed['speed_mean'].dropna(), bins=30, alpha=0.7)\n",
        "axes[0, 0].axvline(df_speed['speed_mean'].mean(), color='red', linestyle='--',\n",
        "                   label=f'Mean: {df_speed[\"speed_mean\"].mean():.2f} mph')\n",
        "axes[0, 0].set_xlabel('Average Speed (mph)')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].set_title('Average Speed Distribution')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Speed consistency distribution\n",
        "axes[0, 1].hist(df_speed['speed_consistency'].dropna(), bins=30, alpha=0.7)\n",
        "axes[0, 1].axvline(df_speed['speed_consistency'].mean(), color='red', linestyle='--',\n",
        "                   label=f'Mean: {df_speed[\"speed_consistency\"].mean():.2f}')\n",
        "axes[0, 1].set_xlabel('Speed Consistency Score')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title('Speed Consistency Distribution')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# High speed ratio distribution\n",
        "axes[0, 2].hist(df_speed['high_speed_ratio'].dropna(), bins=30, alpha=0.7)\n",
        "axes[0, 2].axvline(df_speed['high_speed_ratio'].mean(), color='red', linestyle='--',\n",
        "                   label=f'Mean: {df_speed[\"high_speed_ratio\"].mean():.2f}')\n",
        "axes[0, 2].set_xlabel('High Speed Ratio')\n",
        "axes[0, 2].set_ylabel('Frequency')\n",
        "axes[0, 2].set_title('High Speed Driving Ratio')\n",
        "axes[0, 2].legend()\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "# Speed percentiles comparison\n",
        "speed_percentiles = ['speed_p50', 'speed_p75', 'speed_p90', 'speed_p95', 'speed_p99']\n",
        "percentile_data = df_speed[speed_percentiles]\n",
        "box_data = [percentile_data[col].dropna() for col in speed_percentiles]\n",
        "axes[1, 0].boxplot(box_data, labels=['50%', '75%', '90%', '95%', '99%'])\n",
        "axes[1, 0].set_ylabel('Speed (mph)')\n",
        "axes[1, 0].set_title('Speed Percentile Distribution')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Correlation heatmap\n",
        "speed_corr_features = ['speed_mean', 'speed_std', 'speed_cv', 'speed_consistency',\n",
        "                       'high_speed_ratio', 'speed_p90', 'speed_p95']\n",
        "speed_corr = df_speed[speed_corr_features].corr()\n",
        "sns.heatmap(speed_corr, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "            square=True, ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Speed Feature Correlations')\n",
        "\n",
        "# Speed variability vs consistency\n",
        "scatter = axes[1, 2].scatter(df_speed['speed_std'], df_speed['speed_consistency'],\n",
        "                             alpha=0.6, c=df_speed['speed_cv'], cmap='coolwarm')\n",
        "axes[1, 2].set_xlabel('Speed Standard Deviation')\n",
        "axes[1, 2].set_ylabel('Speed Consistency Score')\n",
        "axes[1, 2].set_title('Speed Variability vs Consistency')\n",
        "plt.colorbar(scatter, ax=axes[1, 2], label='Coefficient of Variation')\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../results/features/speed_features_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 5. RPM Feature Extraction\n",
        "\n",
        "# %%\n",
        "# Extract RPM features\n",
        "print(\"Extracting RPM features...\")\n",
        "df_rpm = extractor.extract_rpm_features(df_speed)\n",
        "\n",
        "# Display RPM features\n",
        "rpm_features = [col for col in df_rpm.columns if 'rpm' in col.lower() and col not in df.columns]\n",
        "print(f\"\\nRPM features created: {len(rpm_features)}\")\n",
        "print(\"\\nSample RPM features:\")\n",
        "print(df_rpm[rpm_features].describe().round(3).T.head(10))\n",
        "\n",
        "# %%\n",
        "# Visualize RPM features\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "# Average RPM distribution\n",
        "axes[0, 0].hist(df_rpm['rpm_mean'].dropna(), bins=30, alpha=0.7)\n",
        "axes[0, 0].axvline(df_rpm['rpm_mean'].mean(), color='red', linestyle='--',\n",
        "                   label=f'Mean: {df_rpm[\"rpm_mean\"].mean():.0f} RPM')\n",
        "axes[0, 0].axvline(2000, color='green', linestyle='-', alpha=0.5, label='Optimal Min (2000)')\n",
        "axes[0, 0].axvline(2500, color='green', linestyle='-', alpha=0.5, label='Optimal Max (2500)')\n",
        "axes[0, 0].set_xlabel('Average RPM')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].set_title('Average RPM Distribution')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# RPM efficiency score distribution\n",
        "axes[0, 1].hist(df_rpm['rpm_efficiency_score'].dropna(), bins=30, alpha=0.7)\n",
        "axes[0, 1].axvline(df_rpm['rpm_efficiency_score'].mean(), color='red', linestyle='--',\n",
        "                   label=f'Mean: {df_rpm[\"rpm_efficiency_score\"].mean():.2f}')\n",
        "axes[0, 1].set_xlabel('RPM Efficiency Score')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title('RPM Efficiency Distribution')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# RPM variability distribution\n",
        "axes[0, 2].hist(df_rpm['rpm_std'].dropna(), bins=30, alpha=0.7)\n",
        "axes[0, 2].axvline(df_rpm['rpm_std'].mean(), color='red', linestyle='--',\n",
        "                   label=f'Mean: {df_rpm[\"rpm_std\"].mean():.0f}')\n",
        "axes[0, 2].set_xlabel('RPM Standard Deviation')\n",
        "axes[0, 2].set_ylabel('Frequency')\n",
        "axes[0, 2].set_title('RPM Variability Distribution')\n",
        "axes[0, 2].legend()\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "# RPM range distribution\n",
        "axes[1, 0].hist(df_rpm['rpm_range'].dropna(), bins=30, alpha=0.7)\n",
        "axes[1, 0].axvline(df_rpm['rpm_range'].mean(), color='red', linestyle='--',\n",
        "                   label=f'Mean: {df_rpm[\"rpm_range\"].mean():.0f}')\n",
        "axes[1, 0].set_xlabel('RPM Range')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].set_title('RPM Range Distribution')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Correlation heatmap\n",
        "rpm_corr_features = ['rpm_mean', 'rpm_std', 'rpm_range', 'rpm_efficiency_score',\n",
        "                     'rpm_variability_score']\n",
        "rpm_corr = df_rpm[rpm_corr_features].corr()\n",
        "sns.heatmap(rpm_corr, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "            square=True, ax=axes[1, 1])\n",
        "axes[1, 1].set_title('RPM Feature Correlations')\n",
        "\n",
        "# RPM efficiency vs variability\n",
        "scatter = axes[1, 2].scatter(df_rpm['rpm_efficiency_score'], df_rpm['rpm_variability_score'],\n",
        "                             alpha=0.6, c=df_rpm['rpm_mean'], cmap='plasma')\n",
        "axes[1, 2].set_xlabel('RPM Efficiency Score')\n",
        "axes[1, 2].set_ylabel('RPM Variability Score')\n",
        "axes[1, 2].set_title('RPM Efficiency vs Variability')\n",
        "plt.colorbar(scatter, ax=axes[1, 2], label='Average RPM')\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../results/features/rpm_features_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 6. Composite Feature Engineering\n",
        "\n",
        "# %%\n",
        "# Create composite features\n",
        "print(\"Creating composite features...\")\n",
        "df_composite = extractor.extract_composite_features(df_rpm)\n",
        "\n",
        "# Display composite features\n",
        "composite_features = [col for col in df_composite.columns if any(x in col for x in ['score', 'index', 'style', 'composite'])]\n",
        "print(f\"\\nComposite features created: {len(composite_features)}\")\n",
        "print(\"\\nComposite features:\")\n",
        "print(df_composite[composite_features].describe().round(3).T)\n",
        "\n",
        "# %%\n",
        "# Visualize composite features\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "# Safety score distribution\n",
        "axes[0, 0].hist(df_composite['safety_score'].dropna(), bins=30, alpha=0.7)\n",
        "axes[0, 0].axvline(df_composite['safety_score'].mean(), color='red', linestyle='--',\n",
        "                   label=f'Mean: {df_composite[\"safety_score\"].mean():.2f}')\n",
        "axes[0, 0].set_xlabel('Safety Score')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].set_title('Safety Score Distribution')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Fuel efficiency composite distribution\n",
        "axes[0, 1].hist(df_composite['fuel_efficiency_composite'].dropna(), bins=30, alpha=0.7)\n",
        "axes[0, 1].axvline(df_composite['fuel_efficiency_composite'].mean(), color='red', linestyle='--',\n",
        "                   label=f'Mean: {df_composite[\"fuel_efficiency_composite\"].mean():.2f}')\n",
        "axes[0, 1].set_xlabel('Fuel Efficiency Composite Score')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title('Fuel Efficiency Distribution')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Aggressive index distribution\n",
        "axes[0, 2].hist(df_composite['aggressive_index'].dropna(), bins=30, alpha=0.7)\n",
        "axes[0, 2].axvline(df_composite['aggressive_index'].mean(), color='red', linestyle='--',\n",
        "                   label=f'Mean: {df_composite[\"aggressive_index\"].mean():.2f}')\n",
        "axes[0, 2].set_xlabel('Aggressive Index')\n",
        "axes[0, 2].set_ylabel('Frequency')\n",
        "axes[0, 2].set_title('Aggressive Driving Index')\n",
        "axes[0, 2].legend()\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "# Driver style distribution\n",
        "style_counts = df_composite['driver_style'].value_counts()\n",
        "axes[1, 0].bar(range(len(style_counts)), style_counts.values)\n",
        "axes[1, 0].set_xticks(range(len(style_counts)))\n",
        "axes[1, 0].set_xticklabels(style_counts.index, rotation=45, ha='right')\n",
        "axes[1, 0].set_ylabel('Number of Drivers')\n",
        "axes[1, 0].set_title('Driver Style Classification')\n",
        "for i, v in enumerate(style_counts.values):\n",
        "    axes[1, 0].text(i, v, str(v), ha='center', va='bottom')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Correlation heatmap\n",
        "composite_corr_features = ['safety_score', 'fuel_efficiency_composite',\n",
        "                           'aggressive_index', 'harsh_accel_count',\n",
        "                           'harsh_brake_count', 'speed_p90']\n",
        "composite_corr = df_composite[composite_corr_features].corr()\n",
        "sns.heatmap(composite_corr, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "            square=True, ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Composite Feature Correlations')\n",
        "\n",
        "# Safety vs Efficiency scatter\n",
        "styles = df_composite['driver_style'].unique()\n",
        "colors = plt.cm.Set3(np.linspace(0, 1, len(styles)))\n",
        "for style, color in zip(styles, colors):\n",
        "    mask = df_composite['driver_style'] == style\n",
        "    axes[1, 2].scatter(df_composite.loc[mask, 'safety_score'],\n",
        "                       df_composite.loc[mask, 'fuel_efficiency_composite'],\n",
        "                       alpha=0.6, label=style, color=color, s=50)\n",
        "axes[1, 2].set_xlabel('Safety Score')\n",
        "axes[1, 2].set_ylabel('Fuel Efficiency Score')\n",
        "axes[1, 2].set_title('Safety vs Fuel Efficiency')\n",
        "axes[1, 2].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../results/features/composite_features_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 7. Feature Selection and Correlation Analysis\n",
        "\n",
        "# %%\n",
        "# Select key features for further analysis\n",
        "key_features = [\n",
        "    # Safety metrics\n",
        "    'safety_score',\n",
        "    'harsh_accel_count',\n",
        "    'harsh_brake_count',\n",
        "    'very_harsh_accel_count',\n",
        "    'very_harsh_brake_count',\n",
        "\n",
        "    # Efficiency metrics\n",
        "    'fuel_efficiency_composite',\n",
        "    'rpm_efficiency_score',\n",
        "    'smooth_driving_score',\n",
        "    'speed_consistency',\n",
        "    'time_consistency',\n",
        "\n",
        "    # Speed metrics\n",
        "    'speed_mean',\n",
        "    'speed_std',\n",
        "    'speed_p90',\n",
        "    'speed_p95',\n",
        "    'high_speed_ratio',\n",
        "\n",
        "    # Aggressiveness\n",
        "    'aggressive_index',\n",
        "    'accel_aggressiveness',\n",
        "    'decel_aggressiveness',\n",
        "    'jerk_score',\n",
        "\n",
        "    # RPM metrics\n",
        "    'rpm_mean',\n",
        "    'rpm_std',\n",
        "    'rpm_range',\n",
        "\n",
        "    # Time metrics\n",
        "    'time_response_mean',\n",
        "    'time_response_std',\n",
        "    'time_p95'\n",
        "]\n",
        "\n",
        "# Filter to available features\n",
        "available_features = [f for f in key_features if f in df_composite.columns]\n",
        "print(f\"Selected {len(available_features)} key features for analysis\")\n",
        "\n",
        "# %%\n",
        "# Create comprehensive correlation matrix\n",
        "print(\"Creating correlation matrix...\")\n",
        "correlation_matrix = df_composite[available_features].corr()\n",
        "\n",
        "# Plot correlation heatmap\n",
        "plt.figure(figsize=(16, 14))\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "sns.heatmap(correlation_matrix, mask=mask, annot=False, cmap='coolwarm',\n",
        "            center=0, square=True, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Comprehensive Feature Correlation Matrix', fontsize=16, pad=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../results/features/comprehensive_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# %%\n",
        "# Identify highly correlated feature pairs\n",
        "print(\"Identifying highly correlated features...\")\n",
        "corr_pairs = []\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > 0.8:  # High correlation threshold\n",
        "            corr_pairs.append((\n",
        "                correlation_matrix.columns[i],\n",
        "                correlation_matrix.columns[j],\n",
        "                correlation_matrix.iloc[i, j]\n",
        "            ))\n",
        "\n",
        "print(f\"\\nFound {len(corr_pairs)} feature pairs with |correlation| > 0.8\")\n",
        "if corr_pairs:\n",
        "    print(\"\\nHighly correlated feature pairs:\")\n",
        "    for pair in sorted(corr_pairs, key=lambda x: abs(x[2]), reverse=True)[:10]:\n",
        "        print(f\"{pair[0]} vs {pair[1]}: {pair[2]:.3f}\")\n",
        "\n",
        "# %%\n",
        "# Feature importance using mutual information (simulated)\n",
        "print(\"\\nCalculating feature importance...\")\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "\n",
        "# Prepare target variable (simulated safety incidents)\n",
        "np.random.seed(42)\n",
        "df_composite['safety_incidents'] = (\n",
        "    (1 - df_composite['safety_score']) * 10 +\n",
        "    df_composite['harsh_accel_count'] * 0.5 +\n",
        "    df_composite['harsh_brake_count'] * 0.5 +\n",
        "    np.random.normal(0, 2, len(df_composite))\n",
        ").clip(0, 20)\n",
        "\n",
        "# Calculate mutual information\n",
        "X = df_composite[available_features].fillna(df_composite[available_features].mean())\n",
        "mi_scores = mutual_info_regression(X, df_composite['safety_incidents'], random_state=42)\n",
        "\n",
        "# Create feature importance DataFrame\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': available_features,\n",
        "    'importance': mi_scores\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(12, 8))\n",
        "bars = plt.barh(range(len(feature_importance)), feature_importance['importance'])\n",
        "plt.yticks(range(len(feature_importance)), feature_importance['feature'])\n",
        "plt.xlabel('Mutual Information Score')\n",
        "plt.title('Feature Importance for Safety Incident Prediction')\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "# Add value labels\n",
        "for i, (bar, importance) in enumerate(zip(bars, feature_importance['importance'])):\n",
        "    plt.text(importance + 0.001, i, f'{importance:.3f}', va='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../results/features/feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 8. Dimensionality Reduction Analysis\n",
        "\n",
        "# %%\n",
        "# Perform PCA for dimensionality reduction\n",
        "print(\"Performing PCA analysis...\")\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df_composite[available_features].fillna(0))\n",
        "\n",
        "# Fit PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Plot explained variance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "         np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
        "plt.axhline(y=0.95, color='r', linestyle='--', alpha=0.5, label='95% Variance')\n",
        "plt.axhline(y=0.90, color='g', linestyle='--', alpha=0.5, label='90% Variance')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA Explained Variance')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "# Annotate specific points\n",
        "for i, (n, var) in enumerate(zip(range(1, 11), np.cumsum(pca.explained_variance_ratio_)[:10])):\n",
        "    if i % 2 == 0:\n",
        "        plt.annotate(f'{var:.1%}', (n, var), textcoords=\"offset points\",\n",
        "                     xytext=(0,10), ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../results/features/pca_explained_variance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# %%\n",
        "# Show top components\n",
        "print(\"\\nPrincipal Components Analysis:\")\n",
        "n_components_95 = np.where(np.cumsum(pca.explained_variance_ratio_) > 0.95)[0][0] + 1\n",
        "n_components_90 = np.where(np.cumsum(pca.explained_variance_ratio_) > 0.90)[0][0] + 1\n",
        "\n",
        "print(f\"Components needed for 90% variance: {n_components_90}\")\n",
        "print(f\"Components needed for 95% variance: {n_components_95}\")\n",
        "\n",
        "# Display top component loadings\n",
        "print(\"\\nTop 5 Principal Component Loadings:\")\n",
        "for i in range(min(5, len(available_features))):\n",
        "    print(f\"\\nPC{i+1} (Explained Variance: {pca.explained_variance_ratio_[i]:.2%}):\")\n",
        "    # Get top 10 features for this PC\n",
        "    loadings = pd.DataFrame({\n",
        "        'feature': available_features,\n",
        "        'loading': pca.components_[i]\n",
        "    })\n",
        "    top_features = loadings.reindex(loadings.loading.abs().sort_values(ascending=False).index)\n",
        "    print(top_features.head(10).to_string(index=False))\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 9. Feature Engineering Summary\n",
        "\n",
        "# %%\n",
        "# Create feature engineering summary\n",
        "feature_categories = {\n",
        "    'Safety Features': ['safety_score', 'harsh_accel_count', 'harsh_brake_count',\n",
        "                       'very_harsh_accel_count', 'very_harsh_brake_count'],\n",
        "    'Efficiency Features': ['fuel_efficiency_composite', 'rpm_efficiency_score',\n",
        "                          'smooth_driving_score', 'speed_consistency'],\n",
        "    'Speed Features': ['speed_mean', 'speed_std', 'speed_p90', 'speed_p95',\n",
        "                      'high_speed_ratio', 'speed_cv'],\n",
        "    'Aggressiveness Features': ['aggressive_index', 'accel_aggressiveness',\n",
        "                              'decel_aggressiveness', 'jerk_score'],\n",
        "    'RPM Features': ['rpm_mean', 'rpm_std', 'rpm_range', 'rpm_variability_score'],\n",
        "    'Time Features': ['time_response_mean', 'time_response_std', 'time_consistency',\n",
        "                     'time_p95', 'time_p99'],\n",
        "    'Composite Features': ['driver_style']\n",
        "}\n",
        "\n",
        "# Filter to available features in each category\n",
        "available_categories = {}\n",
        "for category, features in feature_categories.items():\n",
        "    available = [f for f in features if f in df_composite.columns]\n",
        "    if available:\n",
        "        available_categories[category] = available\n",
        "\n",
        "# Display summary\n",
        "print(\"=\"*60)\n",
        "print(\"FEATURE ENGINEERING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nTotal features created: {len(df_composite.columns) - len(df.columns)}\")\n",
        "print(f\"Total features available: {len(df_composite.columns)}\")\n",
        "\n",
        "print(\"\\nFeature Categories:\")\n",
        "for category, features in available_categories.items():\n",
        "    print(f\"\\n{category} ({len(features)} features):\")\n",
        "    print(f\"  {', '.join(features[:5])}{'...' if len(features) > 5 else ''}\")\n",
        "\n",
        "# Statistical summary of key features\n",
        "print(\"\\n\\nKey Feature Statistics:\")\n",
        "key_stats_features = []\n",
        "for features in available_categories.values():\n",
        "    key_stats_features.extend(features[:2])  # Take first 2 from each category\n",
        "\n",
        "key_stats = df_composite[key_stats_features].describe().T[['mean', 'std', 'min', '50%', 'max']]\n",
        "print(key_stats.round(3))\n",
        "\n",
        "# %%\n",
        "# Save engineered features\n",
        "print(\"\\nSaving engineered features...\")\n",
        "output_path = '../data/driver_features_engineered.csv'\n",
        "df_composite.to_csv(output_path, index=False)\n",
        "print(f\"Engineered features saved to: {output_path}\")\n",
        "\n",
        "# Save feature metadata\n",
        "feature_metadata = {\n",
        "    'total_features': len(df_composite.columns),\n",
        "    'engineered_features': len(df_composite.columns) - len(df.columns),\n",
        "    'feature_categories': available_categories,\n",
        "    'key_features': available_features,\n",
        "    'correlation_insights': {\n",
        "        'highly_correlated_pairs': corr_pairs[:10],\n",
        "        'feature_importance_top10': feature_importance.head(10).to_dict()\n",
        "    },\n",
        "    'pca_insights': {\n",
        "        'components_90_variance': int(n_components_90),\n",
        "        'components_95_variance': int(n_components_95),\n",
        "        'total_variance_explained': float(np.sum(pca.explained_variance_ratio_))\n",
        "    }\n",
        "}\n",
        "\n",
        "import json\n",
        "metadata_path = '../results/features/feature_metadata.json'\n",
        "with open(metadata_path, 'w') as f:\n",
        "    json.dump(feature_metadata, f, indent=2, default=str)\n",
        "print(f\"Feature metadata saved to: {metadata_path}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 10. Next Steps\n",
        "\n",
        "# %%\n",
        "print(\"=\"*60)\n",
        "print(\"NEXT STEPS FOR ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n1. Feature Selection for Clustering:\")\n",
        "print(\"   - Remove highly correlated features\")\n",
        "print(\"   - Select top features based on importance\")\n",
        "print(\"   - Consider domain knowledge\")\n",
        "\n",
        "print(\"\\n2. Data Preparation:\")\n",
        "print(\"   - Handle missing values\")\n",
        "print(\"   - Normalize/standardize features\")\n",
        "print(\"   - Consider outlier treatment\")\n",
        "\n",
        "print(\"\\n3. Clustering Analysis:\")\n",
        "print(\"   - Determine optimal number of clusters\")\n",
        "print(\"   - Apply K-means or other algorithms\")\n",
        "print(\"   - Validate cluster quality\")\n",
        "\n",
        "print(\"\\n4. Interpretation:\")\n",
        "print(\"   - Profile each cluster\")\n",
        "print(\"   - Identify risk patterns\")\n",
        "print(\"   - Develop driver personas\")\n",
        "\n",
        "print(\"\\n5. Actionable Insights:\")\n",
        "print(\"   - Design targeted interventions\")\n",
        "print(\"   - Create training programs\")\n",
        "print(\"   - Develop monitoring dashboards\")"
      ]
    }
  ]
}