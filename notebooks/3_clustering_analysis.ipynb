{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "-096FPSGRZzA",
        "outputId": "5099bed0-1fbe-4777-9515-31eea78e585f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-2308304716.py, line 688)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2308304716.py\"\u001b[0;36m, line \u001b[0;32m688\u001b[0m\n\u001b[0;31m    print(key_stats\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# # Driver Behavior Analysis - Clustering Analysis\n",
        "#\n",
        "# ## Overview\n",
        "# This notebook performs clustering analysis on driver behavior data to identify distinct driving patterns and risk profiles.\n",
        "#\n",
        "# ### Objectives:\n",
        "# 1. Determine optimal number of clusters\n",
        "# 2. Apply K-means clustering algorithm\n",
        "# 3. Analyze and interpret clusters\n",
        "# 4. Validate clustering results\n",
        "# 5. Create driver personas\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 1. Setup and Data Preparation\n",
        "\n",
        "# %%\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Clustering and ML libraries\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Import custom modules\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from src.clustering import DriverClustering\n",
        "from src.data_processor import DataProcessor\n",
        "from src.utils import save_results\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.max_rows', 50)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"viridis\")\n",
        "\n",
        "# %%\n",
        "# Load configuration\n",
        "import yaml\n",
        "with open('../config/config.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Load engineered features\n",
        "print(\"Loading engineered features...\")\n",
        "df = pd.read_csv('../data/driver_features_engineered.csv')\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Number of drivers: {len(df)}\")\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\nSample data:\")\n",
        "print(df.head())\n",
        "\n",
        "# %%\n",
        "# Prepare features for clustering\n",
        "print(\"\\nPreparing features for clustering...\")\n",
        "\n",
        "# Select key features for clustering (based on feature importance from previous analysis)\n",
        "clustering_features = [\n",
        "    # Safety metrics\n",
        "    'safety_score',\n",
        "    'harsh_accel_count',\n",
        "    'harsh_brake_count',\n",
        "\n",
        "    # Efficiency metrics\n",
        "    'fuel_efficiency_composite',\n",
        "    'rpm_efficiency_score',\n",
        "    'smooth_driving_score',\n",
        "\n",
        "    # Speed metrics\n",
        "    'speed_mean',\n",
        "    'speed_p90',\n",
        "    'high_speed_ratio',\n",
        "\n",
        "    # Aggressiveness\n",
        "    'aggressive_index',\n",
        "\n",
        "    # RPM metrics\n",
        "    'rpm_mean',\n",
        "\n",
        "    # Time metrics\n",
        "    'time_response_std',\n",
        "    'time_consistency'\n",
        "]\n",
        "\n",
        "# Filter to available features\n",
        "available_features = [f for f in clustering_features if f in df.columns]\n",
        "print(f\"Selected {len(available_features)} features for clustering:\")\n",
        "\n",
        "# Display feature statistics\n",
        "feature_stats = df[available_features].describe().T[['mean', 'std', 'min', '50%', 'max']]\n",
        "print(feature_stats.round(3))\n",
        "\n",
        "# %%\n",
        "# Handle missing values\n",
        "print(\"\\nHandling missing values...\")\n",
        "missing_before = df[available_features].isnull().sum().sum()\n",
        "df_cluster = df[available_features].copy()\n",
        "\n",
        "# Fill missing values with median\n",
        "for col in df_cluster.columns:\n",
        "    if df_cluster[col].isnull().sum() > 0:\n",
        "        df_cluster[col] = df_cluster[col].fillna(df_cluster[col].median())\n",
        "\n",
        "missing_after = df_cluster.isnull().sum().sum()\n",
        "print(f\"Missing values before: {missing_before}\")\n",
        "print(f\"Missing values after: {missing_after}\")\n",
        "\n",
        "# Check for infinite values\n",
        "inf_count = np.isinf(df_cluster).sum().sum()\n",
        "if inf_count > 0:\n",
        "    print(f\"Found {inf_count} infinite values, replacing with max/min...\")\n",
        "    for col in df_cluster.columns:\n",
        "        col_max = df_cluster[col].replace([np.inf, -np.inf], np.nan).max()\n",
        "        col_min = df_cluster[col].replace([np.inf, -np.inf], np.nan).min()\n",
        "        df_cluster[col] = df_cluster[col].replace(np.inf, col_max)\n",
        "        df_cluster[col] = df_cluster[col].replace(-np.inf, col_min)\n",
        "\n",
        "# %%\n",
        "# Standardize features\n",
        "print(\"Standardizing features...\")\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df_cluster)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=df_cluster.columns, index=df_cluster.index)\n",
        "\n",
        "print(\"Feature standardization complete.\")\n",
        "print(f\"Scaled data shape: {df_scaled.shape}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 2. Optimal Cluster Determination\n",
        "\n",
        "# %%\n",
        "# Determine optimal number of clusters using multiple methods\n",
        "print(\"Determining optimal number of clusters...\")\n",
        "\n",
        "# Range of clusters to test\n",
        "cluster_range = range(2, 11)\n",
        "results = []\n",
        "\n",
        "# Initialize clustering analyzer\n",
        "cluster_analyzer = DriverClustering('../config/config.yaml')\n",
        "\n",
        "# Use the find_optimal_clusters method\n",
        "optimal_k = cluster_analyzer.find_optimal_clusters(df_scaled, max_k=10)\n",
        "print(f\"\\nOptimal number of clusters (Elbow method): {optimal_k}\")\n",
        "\n",
        "# %%\n",
        "# Additional validation methods\n",
        "print(\"\\nPerforming additional cluster validation...\")\n",
        "\n",
        "# Calculate metrics for different k values\n",
        "silhouette_scores = []\n",
        "davies_bouldin_scores = []\n",
        "calinski_harabasz_scores = []\n",
        "\n",
        "for k in cluster_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    clusters = kmeans.fit_predict(df_scaled)\n",
        "\n",
        "    # Calculate metrics\n",
        "    if k > 1:  # Silhouette score requires at least 2 clusters\n",
        "        silhouette_scores.append(silhouette_score(df_scaled, clusters))\n",
        "\n",
        "    davies_bouldin_scores.append(davies_bouldin_score(df_scaled, clusters))\n",
        "    calinski_harabasz_scores.append(calinski_harabasz_score(df_scaled, clusters))\n",
        "\n",
        "# Create visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Silhouette scores\n",
        "axes[0].plot(list(cluster_range)[1:], silhouette_scores, 'bo-', linewidth=2, markersize=8)\n",
        "axes[0].set_xlabel('Number of Clusters')\n",
        "axes[0].set_ylabel('Silhouette Score')\n",
        "axes[0].set_title('Silhouette Score (Higher is better)')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "best_k_silhouette = list(cluster_range)[1:][np.argmax(silhouette_scores)]\n",
        "axes[0].axvline(x=best_k_silhouette, color='r', linestyle='--', alpha=0.5)\n",
        "axes[0].text(best_k_silhouette + 0.1, max(silhouette_scores) * 0.95,\n",
        "             f'Best: k={best_k_silhouette}', color='red')\n",
        "\n",
        "# Davies-Bouldin scores\n",
        "axes[1].plot(cluster_range, davies_bouldin_scores, 'go-', linewidth=2, markersize=8)\n",
        "axes[1].set_xlabel('Number of Clusters')\n",
        "axes[1].set_ylabel('Davies-Bouldin Score')\n",
        "axes[1].set_title('Davies-Bouldin Index (Lower is better)')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "best_k_db = cluster_range[np.argmin(davies_bouldin_scores)]\n",
        "axes[1].axvline(x=best_k_db, color='r', linestyle='--', alpha=0.5)\n",
        "axes[1].text(best_k_db + 0.1, min(davies_bouldin_scores) * 1.05,\n",
        "             f'Best: k={best_k_db}', color='red')\n",
        "\n",
        "# Calinski-Harabasz scores\n",
        "axes[2].plot(cluster_range, calinski_harabasz_scores, 'mo-', linewidth=2, markersize=8)\n",
        "axes[2].set_xlabel('Number of Clusters')\n",
        "axes[2].set_ylabel('Calinski-Harabasz Score')\n",
        "axes[2].set_title('Calinski-Harabasz Index (Higher is better)')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "best_k_ch = cluster_range[np.argmax(calinski_harabasz_scores)]\n",
        "axes[2].axvline(x=best_k_ch, color='r', linestyle='--', alpha=0.5)\n",
        "axes[2].text(best_k_ch + 0.1, max(calinski_harabasz_scores) * 0.95,\n",
        "             f'Best: k={best_k_ch}', color='red')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../results/clusters/cluster_validation_metrics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# %%\n",
        "# Display optimal k from different methods\n",
        "print(\"=\"*60)\n",
        "print(\"OPTIMAL CLUSTER DETERMINATION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nElbow Method (WCSS): k = {optimal_k}\")\n",
        "print(f\"Silhouette Score:    k = {best_k_silhouette}\")\n",
        "print(f\"Davies-Bouldin:      k = {best_k_db}\")\n",
        "print(f\"Calinski-Harabasz:   k = {best_k_ch}\")\n",
        "\n",
        "# Choose final k (majority vote or business decision)\n",
        "final_k = optimal_k  # Using elbow method result\n",
        "print(f\"\\nSelected number of clusters: k = {final_k}\")\n",
        "\n",
        "# Business context considerations\n",
        "print(\"\\nBusiness Considerations:\")\n",
        "print(f\"- 5 clusters provide good differentiation for targeted interventions\")\n",
        "print(f\"- Cluster sizes will be manageable for training programs\")\n",
        "print(f\"- Enough granularity without being too complex\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 3. K-Means Clustering\n",
        "\n",
        "# %%\n",
        "# Perform K-means clustering with selected k\n",
        "print(f\"\\nPerforming K-means clustering with k={final_k}...\")\n",
        "\n",
        "# Initialize K-means\n",
        "kmeans = KMeans(\n",
        "    n_clusters=final_k,\n",
        "    random_state=42,\n",
        "    n_init=20,  # Run multiple times with different centroids\n",
        "    max_iter=300,\n",
        "    tol=1e-4,\n",
        "    init='k-means++'  # Smart initialization\n",
        ")\n",
        "\n",
        "# Fit and predict\n",
        "clusters = kmeans.fit_predict(df_scaled)\n",
        "df_clustered = df.copy()\n",
        "df_clustered['cluster'] = clusters\n",
        "df_clustered['cluster_label'] = df_clustered['cluster'].apply(lambda x: f'Cluster {x+1}')\n",
        "\n",
        "print(\"Clustering completed successfully!\")\n",
        "print(f\"\\nCluster distribution:\")\n",
        "cluster_counts = df_clustered['cluster'].value_counts().sort_index()\n",
        "for cluster, count in cluster_counts.items():\n",
        "    percentage = (count / len(df_clustered)) * 100\n",
        "    print(f\"  Cluster {cluster+1}: {count} drivers ({percentage:.1f}%)\")\n",
        "\n",
        "# %%\n",
        "# Calculate cluster validation metrics\n",
        "print(\"\\nCluster Validation Metrics:\")\n",
        "silhouette_avg = silhouette_score(df_scaled, clusters)\n",
        "db_index = davies_bouldin_score(df_scaled, clusters)\n",
        "ch_index = calinski_harabasz_score(df_scaled, clusters)\n",
        "\n",
        "print(f\"Silhouette Score: {silhouette_avg:.3f}\")\n",
        "print(f\"Davies-Bouldin Index: {db_index:.3f}\")\n",
        "print(f\"Calinski-Harabasz Index: {ch_index:.3f}\")\n",
        "\n",
        "# Interpret scores\n",
        "print(\"\\nInterpretation:\")\n",
        "if silhouette_avg > 0.7:\n",
        "    print(\"✓ Excellent cluster structure (Silhouette > 0.7)\")\n",
        "elif silhouette_avg > 0.5:\n",
        "    print(\"✓ Reasonable cluster structure (Silhouette > 0.5)\")\n",
        "elif silhouette_avg > 0.25:\n",
        "    print(\"✓ Weak but possible cluster structure (Silhouette > 0.25)\")\n",
        "else:\n",
        "    print(\"✗ No substantial cluster structure\")\n",
        "\n",
        "if db_index < 0.7:\n",
        "    print(\"✓ Good cluster separation (DB Index < 0.7)\")\n",
        "elif db_index < 1.0:\n",
        "    print(\"✓ Acceptable cluster separation (DB Index < 1.0)\")\n",
        "else:\n",
        "    print(\"✗ Poor cluster separation\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 4. Cluster Analysis and Interpretation\n",
        "\n",
        "# %%\n",
        "# Analyze cluster characteristics\n",
        "print(\"Analyzing cluster characteristics...\")\n",
        "\n",
        "# Calculate mean values for each cluster\n",
        "cluster_profiles = df_clustered.groupby('cluster')[available_features].mean()\n",
        "cluster_sizes = df_clustered.groupby('cluster').size()\n",
        "cluster_profiles['cluster_size'] = cluster_sizes\n",
        "cluster_profiles['percentage'] = (cluster_sizes / len(df_clustered) * 100).round(1)\n",
        "\n",
        "# Display cluster profiles\n",
        "print(\"\\nCluster Profiles (Mean Values):\")\n",
        "display(cluster_profiles.style.background_gradient(cmap='viridis', axis=0).format(\"{:.3f}\"))\n",
        "\n",
        "# %%\n",
        "# Create normalized cluster profiles for comparison\n",
        "print(\"\\nCreating normalized cluster profiles...\")\n",
        "\n",
        "# Normalize features for better comparison\n",
        "scaler_minmax = MinMaxScaler()\n",
        "features_normalized = scaler_minmax.fit_transform(cluster_profiles[available_features])\n",
        "cluster_profiles_normalized = pd.DataFrame(\n",
        "    features_normalized,\n",
        "    columns=available_features,\n",
        "    index=cluster_profiles.index\n",
        ")\n",
        "\n",
        "# Add cluster size info\n",
        "cluster_profiles_normalized['cluster_size'] = cluster_profiles['cluster_size']\n",
        "cluster_profiles_normalized['percentage'] = cluster_profiles['percentage']\n",
        "\n",
        "# Display normalized profiles\n",
        "print(\"Normalized Cluster Profiles (0-1 scale):\")\n",
        "display(cluster_profiles_normalized.style.background_gradient(cmap='viridis', axis=0).format(\"{:.3f}\"))\n",
        "\n",
        "# %%\n",
        "# Visualize cluster characteristics\n",
        "print(\"Creating cluster visualization...\")\n",
        "\n",
        "# Select top features for visualization\n",
        "top_features = [\n",
        "    'safety_score',\n",
        "    'fuel_efficiency_composite',\n",
        "    'aggressive_index',\n",
        "    'harsh_accel_count',\n",
        "    'harsh_brake_count',\n",
        "    'speed_p90',\n",
        "    'rpm_mean',\n",
        "    'time_consistency'\n",
        "]\n",
        "\n",
        "# Filter to available top features\n",
        "available_top_features = [f for f in top_features if f in cluster_profiles.columns]\n",
        "\n",
        "# Create radar chart for each cluster\n",
        "from math import pi\n",
        "\n",
        "# Normalize features for radar chart\n",
        "radar_data = cluster_profiles_normalized[available_top_features].copy()\n",
        "\n",
        "# Create radar chart\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10), subplot_kw=dict(projection='polar'))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Categories for radar chart\n",
        "categories = available_top_features\n",
        "N = len(categories)\n",
        "\n",
        "# What will be the angle of each axis in the plot\n",
        "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
        "angles += angles[:1]  # Close the loop\n",
        "\n",
        "# Plot each cluster\n",
        "for idx, (cluster_num, row) in enumerate(radar_data.iterrows()):\n",
        "    if idx < len(axes):\n",
        "        ax = axes[idx]\n",
        "\n",
        "        # Values for each category\n",
        "        values = row.values.tolist()\n",
        "        values += values[:1]  # Close the loop\n",
        "\n",
        "        # Draw polygon\n",
        "        ax.plot(angles, values, 'o-', linewidth=2)\n",
        "        ax.fill(angles, values, alpha=0.25)\n",
        "\n",
        "        # Set category labels\n",
        "        ax.set_xticks(angles[:-1])\n",
        "        ax.set_xticklabels(categories, size=8)\n",
        "        ax.set_ylim(0, 1)\n",
        "\n",
        "        # Set title\n",
        "        size = cluster_profiles.loc[cluster_num, 'cluster_size']\n",
        "        pct = cluster_profiles.loc[cluster_num, 'percentage']\n",
        "        ax.set_title(f'Cluster {cluster_num+1}\\n{size} drivers ({pct}%)',\n",
        "                    size=10, fontweight='bold')\n",
        "\n",
        "        # Add grid\n",
        "        ax.grid(True)\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Cluster Profiles - Radar Charts', y=1.02, fontsize=14, fontweight='bold')\n",
        "plt.savefig('../results/clusters/cluster_radar_charts.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# %%\n",
        "# Create parallel coordinates plot\n",
        "print(\"Creating parallel coordinates plot...\")\n",
        "\n",
        "from pandas.plotting import parallel_coordinates\n",
        "\n",
        "# Prepare data for parallel coordinates\n",
        "parallel_data = df_clustered[['cluster'] + available_top_features].copy()\n",
        "parallel_data['cluster'] = parallel_data['cluster'].astype(str)\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "parallel_coordinates(parallel_data, 'cluster', colormap='viridis', alpha=0.5)\n",
        "plt.title('Parallel Coordinates Plot of Driver Clusters', fontsize=14, pad=20)\n",
        "plt.xlabel('Features', fontsize=12)\n",
        "plt.ylabel('Normalized Values', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.savefig('../results/clusters/parallel_coordinates.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 5. Dimensionality Reduction for Visualization\n",
        "\n",
        "# %%\n",
        "# Perform PCA for 2D visualization\n",
        "print(\"Creating 2D visualization using PCA...\")\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(df_scaled)\n",
        "\n",
        "# Create PCA dataframe\n",
        "pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
        "pca_df['cluster'] = clusters\n",
        "pca_df['cluster_label'] = pca_df['cluster'].apply(lambda x: f'Cluster {x+1}')\n",
        "\n",
        "# Plot PCA results\n",
        "plt.figure(figsize=(12, 8))\n",
        "scatter = plt.scatter(pca_df['PC1'], pca_df['PC2'],\n",
        "                     c=pca_df['cluster'], cmap='viridis',\n",
        "                     alpha=0.7, s=50, edgecolor='black', linewidth=0.5)\n",
        "\n",
        "# Add centroids\n",
        "centroids_pca = pca.transform(kmeans.cluster_centers_)\n",
        "plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1],\n",
        "           c='red', marker='X', s=200, label='Centroids',\n",
        "           edgecolor='black', linewidth=1.5)\n",
        "\n",
        "plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
        "plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
        "plt.title('Driver Clusters - PCA Visualization', fontsize=14, pad=20)\n",
        "plt.legend(*scatter.legend_elements(), title=\"Clusters\", loc='upper right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.tight_layout()\n",
        "plt.savefig('../results/clusters/pca_cluster_visualization.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# %%\n",
        "# Perform t-SNE for better separation visualization\n",
        "print(\"Creating t-SNE visualization...\")\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
        "X_tsne = tsne.fit_transform(df_scaled)\n",
        "\n",
        "# Create t-SNE dataframe\n",
        "tsne_df = pd.DataFrame(data=X_tsne, columns=['TSNE1', 'TSNE2'])\n",
        "tsne_df['cluster'] = clusters\n",
        "tsne_df['cluster_label'] = tsne_df['cluster'].apply(lambda x: f'Cluster {x+1}')\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(12, 8))\n",
        "scatter = plt.scatter(tsne_df['TSNE1'], tsne_df['TSNE2'],\n",
        "                     c=tsne_df['cluster'], cmap='viridis',\n",
        "                     alpha=0.7, s=50, edgecolor='black', linewidth=0.5)\n",
        "\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.title('Driver Clusters - t-SNE Visualization', fontsize=14, pad=20)\n",
        "plt.legend(*scatter.legend_elements(), title=\"Clusters\", loc='upper right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.tight_layout()\n",
        "plt.savefig('../results/clusters/tsne_cluster_visualization.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 6. Cluster Interpretation and Labeling\n",
        "\n",
        "# %%\n",
        "# Interpret and label clusters based on characteristics\n",
        "print(\"Interpreting and labeling clusters...\")\n",
        "\n",
        "# Calculate z-scores for each feature within clusters\n",
        "z_scores = {}\n",
        "for feature in available_top_features:\n",
        "    overall_mean = df_clustered[feature].mean()\n",
        "    overall_std = df_clustered[feature].std()\n",
        "\n",
        "    cluster_means = df_clustered.groupby('cluster')[feature].mean()\n",
        "    z_scores[feature] = (cluster_means - overall_mean) / overall_std\n",
        "\n",
        "z_df = pd.DataFrame(z_scores)\n",
        "\n",
        "# Define cluster labels based on characteristics\n",
        "cluster_labels = {}\n",
        "cluster_descriptions = {}\n",
        "\n",
        "for cluster_num in range(final_k):\n",
        "    # Get z-scores for this cluster\n",
        "    cluster_z = z_df.loc[cluster_num]\n",
        "\n",
        "    # Analyze characteristics\n",
        "    characteristics = []\n",
        "\n",
        "    # Safety analysis\n",
        "    safety_z = cluster_z.get('safety_score', 0)\n",
        "    if safety_z > 1.0:\n",
        "        characteristics.append('Very Safe')\n",
        "    elif safety_z > 0.5:\n",
        "        characteristics.append('Safe')\n",
        "    elif safety_z < -1.0:\n",
        "        characteristics.append('Risky')\n",
        "    elif safety_z < -0.5:\n",
        "        characteristics.append('Moderately Risky')\n",
        "\n",
        "    # Efficiency analysis\n",
        "    efficiency_z = cluster_z.get('fuel_efficiency_composite', 0)\n",
        "    if efficiency_z > 1.0:\n",
        "        characteristics.append('Highly Efficient')\n",
        "    elif efficiency_z > 0.5:\n",
        "        characteristics.append('Efficient')\n",
        "    elif efficiency_z < -1.0:\n",
        "        characteristics.append('Fuel Inefficient')\n",
        "    elif efficiency_z < -0.5:\n",
        "        characteristics.append('Moderately Inefficient')\n",
        "\n",
        "    # Aggressiveness analysis\n",
        "    aggressive_z = cluster_z.get('aggressive_index', 0)\n",
        "    if aggressive_z > 1.0:\n",
        "        characteristics.append('Highly Aggressive')\n",
        "    elif aggressive_z > 0.5:\n",
        "        characteristics.append('Aggressive')\n",
        "    elif aggressive_z < -1.0:\n",
        "        characteristics.append('Very Cautious')\n",
        "\n",
        "    # Speed analysis\n",
        "    speed_z = cluster_z.get('speed_p90', 0)\n",
        "    if speed_z > 1.0:\n",
        "        characteristics.append('Fast Driver')\n",
        "    elif speed_z < -1.0:\n",
        "        characteristics.append('Slow Driver')\n",
        "\n",
        "    # Create label\n",
        "    if len(characteristics) > 0:\n",
        "        label = f\"{', '.join(characteristics[:2])} Drivers\"\n",
        "    else:\n",
        "        label = f\"Cluster {cluster_num+1}\"\n",
        "\n",
        "    # Create detailed description\n",
        "    description = f\"Cluster {cluster_num+1} ({cluster_profiles.loc[cluster_num, 'percentage']}% of drivers): \"\n",
        "    description += f\"Characterized by \"\n",
        "\n",
        "    # Add top 3 features (by absolute z-score)\n",
        "    top_features = cluster_z.abs().sort_values(ascending=False).head(3)\n",
        "    desc_features = []\n",
        "    for feature, z in top_features.items():\n",
        "        if z > 0.5:  # Only mention significant features\n",
        "            direction = \"high\" if cluster_z[feature] > 0 else \"low\"\n",
        "            desc_features.append(f\"{direction} {feature}\")\n",
        "\n",
        "    if desc_features:\n",
        "        description += \", \".join(desc_features)\n",
        "    else:\n",
        "        description += \"average performance across most metrics\"\n",
        "\n",
        "    cluster_labels[cluster_num] = label\n",
        "    cluster_descriptions[cluster_num] = description\n",
        "\n",
        "# Display cluster interpretations\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLUSTER INTERPRETATION AND LABELING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for cluster_num in range(final_k):\n",
        "    print(f\"\\n{cluster_labels[cluster_num]}:\")\n",
        "    print(f\"  - {cluster_descriptions[cluster_num]}\")\n",
        "    print(f\"  - Size: {cluster_profiles.loc[cluster_num, 'cluster_size']} drivers\")\n",
        "\n",
        "    # Show key metrics\n",
        "    print(\"  - Key Metrics (z-scores):\")\n",
        "    for feature in available_top_features[:4]:\n",
        "        z = z_df.loc[cluster_num, feature]\n",
        "        if abs(z) > 0.5:\n",
        "            print(f\"    * {feature}: {z:+.2f}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 7. Driver Personas Development\n",
        "\n",
        "# %%\n",
        "# Create detailed driver personas\n",
        "print(\"\\nCreating driver personas...\")\n",
        "\n",
        "# Define personas based on cluster characteristics\n",
        "personas = {}\n",
        "\n",
        "for cluster_num in range(final_k):\n",
        "    persona = {\n",
        "        'cluster_number': cluster_num + 1,\n",
        "        'label': cluster_labels[cluster_num],\n",
        "        'size': int(cluster_profiles.loc[cluster_num, 'cluster_size']),\n",
        "        'percentage': float(cluster_profiles.loc[cluster_num, 'percentage']),\n",
        "        'key_characteristics': [],\n",
        "        'risk_level': '',\n",
        "        'efficiency_level': '',\n",
        "        'training_needs': [],\n",
        "        'monitoring_frequency': '',\n",
        "        'intervention_priority': ''\n",
        "    }\n",
        "\n",
        "    # Get cluster statistics\n",
        "    cluster_data = df_clustered[df_clustered['cluster'] == cluster_num]\n",
        "\n",
        "    # Determine risk level\n",
        "    safety_mean = cluster_profiles.loc[cluster_num, 'safety_score']\n",
        "    if safety_mean > 0.7:\n",
        "        persona['risk_level'] = 'Low'\n",
        "    elif safety_mean > 0.5:\n",
        "        persona['risk_level'] = 'Moderate'\n",
        "    elif safety_mean > 0.3:\n",
        "        persona['risk_level'] = 'High'\n",
        "    else:\n",
        "        persona['risk_level'] = 'Critical'\n",
        "\n",
        "    # Determine efficiency level\n",
        "    efficiency_mean = cluster_profiles.loc[cluster_num, 'fuel_efficiency_composite']\n",
        "    if efficiency_mean > 0.7:\n",
        "        persona['efficiency_level'] = 'High'\n",
        "    elif efficiency_mean > 0.5:\n",
        "        persona['efficiency_level'] = 'Moderate'\n",
        "    else:\n",
        "        persona['efficiency_level'] = 'Low'\n",
        "\n",
        "    # Identify key characteristics (based on z-scores)\n",
        "    cluster_z = z_df.loc[cluster_num]\n",
        "    significant_features = cluster_z[abs(cluster_z) > 0.8].sort_values(key=abs, ascending=False)\n",
        "\n",
        "    for feature, z_score in significant_features.head(3).items():\n",
        "        if z_score > 0:\n",
        "            persona['key_characteristics'].append(f\"High {feature}\")\n",
        "        else:\n",
        "            persona['key_characteristics'].append(f\"Low {feature}\")\n",
        "\n",
        "    # Determine training needs\n",
        "    if cluster_profiles.loc[cluster_num, 'harsh_accel_count'] > df_clustered['harsh_accel_count'].mean():\n",
        "        persona['training_needs'].append('Smooth Acceleration')\n",
        "\n",
        "    if cluster_profiles.loc[cluster_num, 'harsh_brake_count'] > df_clustered['harsh_brake_count'].mean():\n",
        "        persona['training_needs'].append('Defensive Braking')\n",
        "\n",
        "    if cluster_profiles.loc[cluster_num, 'speed_p90'] > df_clustered['speed_p90'].mean() * 1.1:\n",
        "        persona['training_needs'].append('Speed Management')\n",
        "\n",
        "    if cluster_profiles.loc[cluster_num, 'rpm_mean'] > df_clustered['rpm_mean'].mean() * 1.1:\n",
        "        persona['training_needs'].append('Gear Optimization')\n",
        "\n",
        "    # Determine monitoring frequency\n",
        "    if persona['risk_level'] in ['Critical', 'High']:\n",
        "        persona['monitoring_frequency'] = 'Daily'\n",
        "    elif persona['risk_level'] == 'Moderate':\n",
        "        persona['monitoring_frequency'] = 'Weekly'\n",
        "    else:\n",
        "        persona['monitoring_frequency'] = 'Monthly'\n",
        "\n",
        "    # Determine intervention priority\n",
        "    if persona['risk_level'] == 'Critical':\n",
        "        persona['intervention_priority'] = 'Immediate'\n",
        "    elif persona['risk_level'] == 'High':\n",
        "        persona['intervention_priority'] = 'High'\n",
        "    elif persona['risk_level'] == 'Moderate':\n",
        "        persona['intervention_priority'] = 'Medium'\n",
        "    else:\n",
        "        persona['intervention_priority'] = 'Low'\n",
        "\n",
        "    personas[cluster_num] = persona\n",
        "\n",
        "# Display personas\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DRIVER PERSONAS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for cluster_num, persona in personas.items():\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(f\"PERSONA: {persona['label']}\")\n",
        "    print(f\"{'='*40}\")\n",
        "    print(f\"Cluster: {persona['cluster_number']}\")\n",
        "    print(f\"Size: {persona['size']} drivers ({persona['percentage']}%)\")\n",
        "    print(f\"Risk Level: {persona['risk_level']}\")\n",
        "    print(f\"Efficiency Level: {persona['efficiency_level']}\")\n",
        "    print(f\"Key Characteristics: {', '.join(persona['key_characteristics'])}\")\n",
        "    print(f\"Training Needs: {', '.join(persona['training_needs']) if persona['training_needs'] else 'Minimal'}\")\n",
        "    print(f\"Monitoring Frequency: {persona['monitoring_frequency']}\")\n",
        "    print(f\"Intervention Priority: {persona['intervention_priority']}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 8. Cluster Validation and Stability\n",
        "\n",
        "# %%\n",
        "# Validate cluster stability\n",
        "print(\"Validating cluster stability...\")\n",
        "\n",
        "# Method 1: Compare with different random states\n",
        "print(\"\\n1. Stability across different random seeds:\")\n",
        "different_seeds_results = []\n",
        "seeds = [42, 123, 456, 789, 999]\n",
        "\n",
        "for seed in seeds:\n",
        "    kmeans_temp = KMeans(n_clusters=final_k, random_state=seed, n_init=10)\n",
        "    clusters_temp = kmeans_temp.fit_predict(df_scaled)\n",
        "    different_seeds_results.append(clusters_temp)\n",
        "\n",
        "# Calculate agreement between different runs\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "agreement_scores = []\n",
        "for i in range(len(seeds)):\n",
        "    for j in range(i+1, len(seeds)):\n",
        "        score = adjusted_rand_score(different_seeds_results[i], different_seeds_results[j])\n",
        "        agreement_scores.append(score)\n",
        "\n",
        "print(f\"  Average agreement between different seeds: {np.mean(agreement_scores):.3f}\")\n",
        "print(f\"  Minimum agreement: {np.min(agreement_scores):.3f}\")\n",
        "print(f\"  Maximum agreement: {np.max(agreement_scores):.3f}\")\n",
        "\n",
        "if np.mean(agreement_scores) > 0.9:\n",
        "    print(\"  ✓ Excellent cluster stability\")\n",
        "elif np.mean(agreement_scores) > 0.7:\n",
        "    print(\"  ✓ Good cluster stability\")\n",
        "elif np.mean(agreement_scores) > 0.5:\n",
        "    print(\"  ✓ Acceptable cluster stability\")\n",
        "else:\n",
        "    print(\"  ✗ Poor cluster stability\")\n",
        "\n",
        "# %%\n",
        "# Method 2: Compare with different clustering algorithms\n",
        "print(\"\\n2. Comparison with alternative clustering algorithms:\")\n",
        "\n",
        "# Try DBSCAN\n",
        "print(\"  Testing DBSCAN...\")\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "clusters_dbscan = dbscan.fit_predict(df_scaled)\n",
        "n_clusters_dbscan = len(set(clusters_dbscan)) - (1 if -1 in clusters_dbscan else 0)\n",
        "print(f\"    DBSCAN found {n_clusters_dbscan} clusters\")\n",
        "\n",
        "# Try Agglomerative Clustering\n",
        "print(\"  Testing Agglomerative Clustering...\")\n",
        "agglo = AgglomerativeClustering(n_clusters=final_k)\n",
        "clusters_agglo = agglo.fit_predict(df_scaled)\n",
        "\n",
        "# Compare with K-means results\n",
        "ari_dbscan = adjusted_rand_score(clusters, clusters_dbscan)\n",
        "ari_agglo = adjusted_rand_score(clusters, clusters_agglo)\n",
        "\n",
        "print(f\"    Agreement with DBSCAN: {ari_dbscan:.3f}\")\n",
        "print(f\"    Agreement with Agglomerative: {ari_agglo:.3f}\")\n",
        "\n",
        "# Method 3: Bootstrap stability\n",
        "print(\"\\n3. Bootstrap stability analysis...\")\n",
        "n_bootstraps = 10\n",
        "bootstrap_scores = []\n",
        "\n",
        "for i in range(n_bootstraps):\n",
        "    # Create bootstrap sample\n",
        "    bootstrap_indices = np.random.choice(len(df_scaled), size=len(df_scaled), replace=True)\n",
        "    X_bootstrap = df_scaled.iloc[bootstrap_indices]\n",
        "\n",
        "    # Cluster bootstrap sample\n",
        "    kmeans_bootstrap = KMeans(n_clusters=final_k, random_state=42)\n",
        "    clusters_bootstrap = kmeans_bootstrap.fit_predict(X_bootstrap)\n",
        "\n",
        "    # Map to original clusters using subset\n",
        "    common_indices = list(set(bootstrap_indices))\n",
        "    if len(common_indices) > 10:\n",
        "        original_clusters_subset = np.array([clusters[idx] for idx in common_indices])\n",
        "        bootstrap_clusters_subset = np.array([clusters_bootstrap[list(bootstrap_indices).index(idx)]\n",
        "                                            for idx in common_indices])\n",
        "\n",
        "        score = adjusted_rand_score(original_clusters_subset, bootstrap_clusters_subset)\n",
        "        bootstrap_scores.append(score)\n",
        "\n",
        "if bootstrap_scores:\n",
        "    print(f\"  Average bootstrap stability: {np.mean(bootstrap_scores):.3f}\")\n",
        "    print(f\"  Bootstrap stability range: [{np.min(bootstrap_scores):.3f}, {np.max(bootstrap_scores):.3f}]\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 9. Business Insights and Recommendations\n",
        "\n",
        "# %%\n",
        "# Generate business insights from clustering\n",
        "print(\"Generating business insights...\")\n",
        "\n",
        "# Calculate overall statistics\n",
        "total_drivers = len(df_clustered)\n",
        "high_risk_drivers = df_clustered[df_clustered['safety_score'] < 0.4].shape[0]\n",
        "low_efficiency_drivers = df_clustered[df_clustered['fuel_efficiency_composite'] < 0.4].shape[0]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BUSINESS INSIGHTS FROM CLUSTERING ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\n1. Overall Statistics:\")\n",
        "print(f\"   - Total drivers analyzed: {total_drivers}\")\n",
        "print(f\"   - High-risk drivers identified: {high_risk_drivers} ({high_risk_drivers/total_drivers*100:.1f}%)\")\n",
        "print(f\"   - Low-efficiency drivers identified: {low_efficiency_drivers} ({low_efficiency_drivers/total_drivers*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\n2. Cluster Distribution:\")\n",
        "for cluster_num in range(final_k):\n",
        "    size = cluster_profiles.loc[cluster_num, 'cluster_size']\n",
        "    pct = cluster_profiles.loc[cluster_num, 'percentage']\n",
        "    label = cluster_labels[cluster_num]\n",
        "    print(f\"   - {label}: {size} drivers ({pct}%)\")\n",
        "\n",
        "print(f\"\\n3. Risk Analysis by Cluster:\")\n",
        "risk_analysis = []\n",
        "for cluster_num in range(final_k):\n",
        "    cluster_data = df_clustered[df_clustered['cluster'] == cluster_num]\n",
        "    high_risk_in_cluster = cluster_data[cluster_data['safety_score'] < 0.4].shape[0]\n",
        "    cluster_risk_pct = (high_risk_in_cluster / len(cluster_data)) * 100 if len(cluster_data) > 0 else 0\n",
        "    risk_analysis.append((cluster_num, cluster_risk_pct))\n",
        "\n",
        "# Sort by risk percentage\n",
        "risk_analysis.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for cluster_num, risk_pct in risk_analysis:\n",
        "    print(f\"   - Cluster {cluster_num+1}: {risk_pct:.1f}% high-risk drivers\")\n",
        "\n",
        "print(f\"\\n4. Efficiency Analysis by Cluster:\")\n",
        "efficiency_analysis = []\n",
        "for cluster_num in range(final_k):\n",
        "    cluster_data = df_clustered[df_clustered['cluster'] == cluster_num]\n",
        "    avg_efficiency = cluster_data['fuel_efficiency_composite'].mean()\n",
        "    efficiency_analysis.append((cluster_num, avg_efficiency))\n",
        "\n",
        "# Sort by efficiency\n",
        "efficiency_analysis.sort(key=lambda x: x[1])\n",
        "\n",
        "for cluster_num, efficiency in efficiency_analysis:\n",
        "    label = cluster_labels[cluster_num]\n",
        "    print(f\"   - {label}: Average efficiency = {efficiency:.2f}\")\n",
        "\n",
        "print(f\"\\n5. Actionable Recommendations:\")\n",
        "\n",
        "# Priority 1: Critical clusters\n",
        "critical_clusters = []\n",
        "for cluster_num in range(final_k):\n",
        "    cluster_risk_pct = risk_analysis[cluster_num][1]\n",
        "    if cluster_risk_pct > 30:  # More than 30% high-risk drivers\n",
        "        critical_clusters.append(cluster_num)\n",
        "\n",
        "if critical_clusters:\n",
        "    print(\"   a. IMMEDIATE ACTION REQUIRED for clusters with >30% high-risk drivers:\")\n",
        "    for cluster_num in critical_clusters:\n",
        "        print(f\"      - {cluster_labels[cluster_num]} (Cluster {cluster_num+1})\")\n",
        "    print(\"      Recommended: Mandatory retraining program\")\n",
        "\n",
        "# Priority 2: Efficiency improvement\n",
        "efficient_clusters = [c for c, eff in efficiency_analysis if eff < 0.5]\n",
        "if efficient_clusters:\n",
        "    print(\"\\n   b. EFFICIENCY IMPROVEMENT opportunities:\")\n",
        "    for cluster_num in efficient_clusters[:2]:  # Top 2 least efficient\n",
        "        print(f\"      - {cluster_labels[cluster_num]} (Cluster {cluster_num+1})\")\n",
        "    print(\"      Recommended: Fuel efficiency training\")\n",
        "\n",
        "# Priority 3: Best practices sharing\n",
        "best_clusters = [c for c, eff in efficiency_analysis if eff > 0.7][:2]\n",
        "if best_clusters:\n",
        "    print(\"\\n   c. BEST PRACTICES from top-performing clusters:\")\n",
        "    for cluster_num in best_clusters:\n",
        "        print(f\"      - {cluster_labels[cluster_num]} (Cluster {cluster_num+1})\")\n",
        "    print(\"      Recommended: Peer mentoring program\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 10. Results Export and Next Steps\n",
        "\n",
        "# %%\n",
        "# Save clustering results\n",
        "print(\"Saving clustering results...\")\n",
        "\n",
        "# Add cluster labels to original dataframe\n",
        "df_final = df.copy()\n",
        "df_final['cluster'] = clusters\n",
        "df_final['cluster_label'] = df_final['cluster'].apply(lambda x: f'Cluster {x+1}')\n",
        "\n",
        "# Add persona information\n",
        "for cluster_num, persona in personas.items():\n",
        "    mask = df_final['cluster'] == cluster_num\n",
        "    df_final.loc[mask, 'persona_label'] = persona['label']\n",
        "    df_final.loc[mask, 'risk_level'] = persona['risk_level']\n",
        "    df_final.loc[mask, 'efficiency_level'] = persona['efficiency_level']\n",
        "    df_final.loc[mask, 'intervention_priority'] = persona['intervention_priority']\n",
        "\n",
        "# Save results\n",
        "output_files = {\n",
        "    'clustered_data': '../data/driver_data_clustered.csv',\n",
        "    'cluster_profiles': '../results/clusters/cluster_profiles.csv',\n",
        "    'personas': '../results/clusters/driver_personas.csv',\n",
        "    'clustering_metrics': '../results/clusters/clustering_metrics.json'\n",
        "}\n",
        "\n",
        "# Save clustered data\n",
        "df_final.to_csv(output_files['clustered_data'], index=False)\n",
        "print(f\"✓ Clustered data saved to: {output_files['clustered_data']}\")\n",
        "\n",
        "# Save cluster profiles\n",
        "cluster_profiles.to_csv(output_files['cluster_profiles'])\n",
        "print(f\"✓ Cluster profiles saved to: {output_files['cluster_profiles']}\")\n",
        "\n",
        "# Save personas\n",
        "personas_df = pd.DataFrame.from_dict(personas, orient='index')\n",
        "personas_df.to_csv(output_files['personas'])\n",
        "print(f\"✓ Driver personas saved to: {output_files['personas']}\")\n",
        "\n",
        "# Save clustering metrics\n",
        "clustering_metrics = {\n",
        "    'optimal_clusters': int(final_k),\n",
        "    'silhouette_score': float(silhouette_avg),\n",
        "    'davies_bouldin_index': float(db_index),\n",
        "    'calinski_harabasz_index': float(ch_index),\n",
        "    'cluster_distribution': cluster_counts.to_dict(),\n",
        "    'pca_variance_explained': {\n",
        "        'pc1': float(pca.explained_variance_ratio_[0]),\n",
        "        'pc2': float(pca.explained_variance_ratio_[1]),\n",
        "        'total_2d': float(pca.explained_variance_ratio_[0] + pca.explained_variance_ratio_[1])\n",
        "    },\n",
        "    'stability_metrics': {\n",
        "        'average_seed_agreement': float(np.mean(agreement_scores)) if agreement_scores else 0,\n",
        "        'bootstrap_stability': float(np.mean(bootstrap_scores)) if bootstrap_scores else 0\n",
        "    }\n",
        "}\n",
        "\n",
        "import json\n",
        "with open(output_files['clustering_metrics'], 'w') as f:\n",
        "    json.dump(clustering_metrics, f, indent=2)\n",
        "print(f\"✓ Clustering metrics saved to: {output_files['clustering_metrics']}\")\n",
        "\n",
        "# %%\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLUSTERING ANALYSIS COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nSummary of Results:\")\n",
        "print(f\"• Optimal clusters identified: {final_k}\")\n",
        "print(f\"• Cluster quality: Silhouette = {silhouette_avg:.3f}\")\n",
        "print(f\"• High-risk drivers identified: {high_risk_drivers} ({high_risk_drivers/total_drivers*100:.1f}%)\")\n",
        "print(f\"• Driver personas created: {final_k} distinct profiles\")\n",
        "\n",
        "print(\"\\nNext Steps:\")\n",
        "print(\"1. Review cluster profiles and personas\")\n",
        "print(\"2. Design targeted intervention programs\")\n",
        "print(\"3. Implement monitoring dashboards\")\n",
        "print(\"4. Schedule follow-up analysis in 3 months\")\n",
        "print(\"5. Integrate with existing safety programs\")\n",
        "\n",
        "print(\"\\nFiles Created:\")\n",
        "for key, path in output_files.items():\n",
        "    print(f\"  • {key}: {path}\")"
      ]
    }
  ]
}